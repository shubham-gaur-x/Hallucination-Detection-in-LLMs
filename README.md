
# ðŸ§  Hallucination Detection in Large Language Models (LLMs)

This project investigates hallucination detection in Large Language Models (LLMs) using both traditional machine learning and advanced LLM-based methods, enhanced with retrieval-augmented generation (RAG). The core objective is to detect when LLMs confidently generate factually incorrect or unverifiable information â€” a major challenge in trustworthy AI.

---

## ðŸ“Œ Project Highlights

- **Dataset**: [FEVER](https://fever.ai/) â€” 145k+ human-generated claims with labeled evidence.
- **Models Compared**:
  - Traditional ML: Random Forest, XGBoost
  - LLMs: GPT-3.5, Claude (with and without RAG)
- **Key Techniques**:
  - Semantic similarity with Sentence Transformers
  - Named entity overlap with spaCy
  - RAG using FAISS vector store for evidence retrieval
  - Prompt engineering for LLM inference
- **Evaluation**: Class-wise accuracy, false positives, hallucination rate (especially for "Not Enough Info")

---

## ðŸ§¬ Dataset Overview

The FEVER dataset includes three classes:
- `SUPPORTS`: Claim is verified true based on evidence.
- `REFUTES`: Claim is directly contradicted by evidence.
- `NOT ENOUGH INFO`: Claim cannot be verified from available documents.

We performed:
- Class balancing (equal samples across labels)
- Data cleaning and formatting
- Stratified train/val/test split (70/15/15)

---

## ðŸ”§ System Architecture

```text
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   FEVER Dataset    â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚    Feature Engineering        â”‚
         â”‚  - Semantic Similarity        â”‚
         â”‚  - Entity Overlap             â”‚
         â”‚  - Text Length, etc.          â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚       Parallel Model Pipelines  â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
        â”‚  â”‚ ML Models  â”‚ LLMs (w/ RAG)â”‚  â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚ Evaluation &   â”‚
                 â”‚ Hallucination  â”‚
                 â”‚ Detection      â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ“ˆ Performance Summary

| Model                | Accuracy | With RAG | Hallucination Rate vs RF |
|---------------------|----------|----------|---------------------------|
| Random Forest        | 81.49%   | â€”        | â€”                         |
| XGBoost              | 81.88%   | â€”        | â€”                         |
| GPT-3.5 (standard)   | 59.40%   | âŒ       | 87.04%                    |
| GPT-3.5 + RAG        | 66.00%   | âœ… +6.6% | â€”                         |
| Claude (standard)    | 36.60%   | âŒ       | 95.06%                    |
| Claude + RAG         | 47.20%   | âœ… +10.6%| â€”                         |

---

## ðŸš€ How to Run

### 1. Clone the repo
```bash
git clone https://github.com/shubham-gaur-x/Hallucination-Detection-in-LLMs.git
cd Hallucination-Detection-in-LLMs
```

### 2. Setup environment
```bash
pip install -r requirements.txt
```

### 3. Run training pipelines
```bash
# For traditional ML
python train_ml_models.py

# For LLM with RAG
python run_rag_pipeline.py
```

---

## ðŸ§  Key Insights

- **Traditional ML > LLMs** in raw accuracy.
- **RAG significantly improves** LLM performance, especially Claude.
- **REFUTES class** remains hardest for LLMs, indicating limits in contradiction handling.
- Hybrid ML + LLM architectures hold potential for robust hallucination detection.

---

## ðŸ“š Future Work

- Multi-hop RAG for deeper fact chaining
- Hybrid LLM + classifier ensemble systems
- Hallucination-specific loss functions
- Testing on other factual verification datasets (e.g., SciFact)

---

## ðŸ“„ License

This project is for academic/research purposes. Please cite appropriately if used.
